# Experimental MakeCode extension to run ML4F models

[![MakeCode Project](https://github.com/microbit-foundation/pxt-ml-runner-poc/actions/workflows/makecode.yml/badge.svg)](https://github.com/microbit-foundation/pxt-ml-runner-poc/actions/workflows/makecode.yml)
[![Header Generator Tests](https://github.com/microbit-foundation/pxt-ml-runner-poc/actions/workflows/header-gen.yml/badge.svg)](https://github.com/microbit-foundation/pxt-ml-runner-poc/actions/workflows/header-gen.yml)

This project includes [ML4F](https://github.com/microsoft/ml4f) and a wrapper
to invoke a known type of model. It is left slim to be able to be import it
in other MakeCode extensions and a MicroPython module.

## How to use your ML4F model with this extension

This extension expects the classes to be present as a global `MlRunnerLabels`
enum, and the ML4F + custom header to be provided by a `getModelBlob()`
function (no namespaces) as as a hex template literal.

An example for this can  be seen in the [`autogenerated.ts`](autogenerated.ts)
file.

## Developer considerations

- Almost nothing implemented yet
- Currently it embeds a model in a C file as an array with blob data
- Due to the model size, BLE had to be disabled
    - This currently doesn't work on live MakeCode, only beta
        - Actually it doesn't look like it works in beta either, just locally
    - Adding other BLE extensions might fail
- This repository [issue tracker](https://github.com/microbit-foundation/pxt-ml-runner-poc/issues/)
  contains issues that could be encountered and workarounds 
- Only built and tested on micro:bit V2


## Use as Extension

This repository can be added as an **extension** in MakeCode.

* Open [MakeCode beta](https://makecode.microbit.org/beta)
* Click on **New Project**
* Click on **Extensions** under the gearwheel menu
* Search for **https://github.com/microbit-foundation/pxt-ml-runner-poc** and import


## Edit this project

### In MakeCode online editor

To edit this repository in MakeCode.

* Open [MakeCode](https://makecode.microbit.org)
* Click on **Import** then click on **Import URL**
* Paste **https://github.com/microbit-foundation/pxt-ml-runner-poc** and click import

### Building locally

Ensure you have the required toolchain to build for V1 and V2
(arm-none-eabi-gcc, python, yotta, cmake, ninja, srec_cat) or docker.

```bash
git clone https://github.com/microbit-foundation/pxt-ml-runner-poc
cd pxt-ml-runner-poc
npm install pxt --no-save
npx pxt target microbit --no-save
npx pxt install
PXT_FORCE_LOCAL=1 PXT_NODOCKER=1 npx pxt
```

For the V1 build Yotta can hit the GitHub rate limits quite easily if the
project is built from a clean state more than once.
A V2-only build can be performed with the `PXT_COMPILE_SWITCHES=csv---mbcodal`
environmental variable.

```
PXT_FORCE_LOCAL=1 PXT_NODOCKER=1 PXT_COMPILE_SWITCHES=csv---mbcodal npx pxt
```

> [!CAUTION]
> When pushing changes to this repository, do **not** modify the `enums.d.ts`
> and `shims.d.ts` files.
> These are autogenerated by MakeCode to contain the enums and function shims
> from the C++ code to be accessible by the TypeScript code. However, these are
> only needed for the test code, and are not meant to be shipped as part of
> the "normal" extension files.
> Unfortunately, adding `enums.d.ts` and `shims.d.ts` to the `testFiles` entry
> in `pxt.json` does not work, and they need to be added to `files`, so we keep
> them empty. Every build will add code in there, but should those changes 
> should never be pushed.


## Build flags

### Built-in ML model

The `MLRUNNER_USE_EXAMPLE_MODEL` flag can be used to add into a project an
example model included in this extension.

- 0: This is the default behaviour, no built-in module is build at all by
  this extension.
- 1: Includes a ML-Trainer model converted with ML4F. Trained with 3 classes,
  shake, circle and still.
- 2: This will include the Keras ADL model converted with ML4F.
  This model is too large and might not fit in normal builds without excluding
  the BLE SoftDevice, so it's usage is discouraged.
  Classes: Jumping, Running, Standing, Walking

```json
{
    "yotta": {
        "config": {
            "MLRUNNER_USE_EXAMPLE_MODEL": 1
        }
    }
}
```


This flag name is expanded to `DEVICE_MLRUNNER_USE_EXAMPLE_MODEL` in the
source code.


## License
This software is under the MIT open source license.

[SPDX-License-Identifier: MIT](LICENSE)


## Code of Conduct

Trust, partnership, simplicity and passion are our core values we live and
breathe in our daily work life and within our projects. Our open-source
projects are no exception. We have an active community which spans the globe
and we welcome and encourage participation and contributions to our projects
by everyone. We work to foster a positive, open, inclusive and supportive
environment and trust that our community respects the micro:bit code of
conduct. Please see our [code of conduct](https://microbit.org/safeguarding/)
which outlines our expectations for all those that participate in our
community and details on how to report any concerns and what would happen
should breaches occur.


#### Metadata (used for search)

* for PXT/microbit
